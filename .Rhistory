tmp <- fcmat[, as.character(input$sentiment_select)]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
fcm_select(fcmat, pattern = c(tmp, as.character(input$sentiment_select)))
})
output$setsemanPlot <- renderPlot(
textplot_network(fcm_local())
)
}
)
}
set.seed(12345)
load("F:/Bea/PostDoc/RESEARCH/ComTxt/data/example_df.rda")
df <- example_df
matrix <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
library(topicmodels)
## by perperxity
dtm <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
##set the number of topics
## call load packages
library(SparseM)
library(lava)
library(RTextTools)
library(tm)
## make a plot with
library(ggplot2)
library(Rmpfr)
## by perperxity
dtm <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
#Filtering empty rows of the matrix (DTM) - important for performance
rowTotals <- apply(dtm, 1, sum)
dtm_filter <- dtm[rowTotals > 0,]
#Model optimization - calculating perplexity for different values of k
set.seed(12345)
train = sample(rownames(dtm_filter), nrow(dtm_filter) * .75)
dtm_train = dtm_filter[rownames(dtm_filter) %in% train, ]
dtm_test = dtm_filter[!rownames(dtm_filter) %in% train, ]
perplexity = data.frame(k = 2:40, p=NA) #calculating perplexity for k 5:20
for (k in perplexity$k) {
message("k=", k)
m = LDA(dtm_train, method = "Gibbs", k = k,  control = list(alpha = 5/k))
perplexity$p[perplexity$k==k] = perplexity(m, dtm_test)
}
ggplot(perplexity, aes(x = k, y = p)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best topic number by perplexity score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,40,1)) + ylab("perplexity")
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
k_list <- seq(1, 40, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
filename = file.path(model_dir, paste0(k, "_topics.rda"))
if (!file.exists(filename)) {
m <- FitLdaModel(dtm = dtm, k = k, iterations = iter)
m$k <- k
m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
save(m, file = filename)
} else {
load(filename)
}
m
}, export=c("dtm", "model_dir")) # export only needed for Windows machines
#model tuning
#choosing the best model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)),
coherence = sapply(model_list, function(x) mean(x$coherence)),
stringsAsFactors = FALSE)
library(Rmpfr)
## make a plot with
library(ggplot2)
library(grid)
library(gridExtra)
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
#create DTM
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
##set the number of topics
## call load packages
library(SparseM)
library(lava)
library(RTextTools)
library(tm)
library(slam)
library(topicmodels)
library(Rmpfr)
## make a plot with
library(ggplot2)
library(grid)
library(gridExtra)
#create DTM
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
library(textmineR)
#create DTM
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
library(dplyr)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
k_list <- seq(1, 40, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
filename = file.path(model_dir, paste0(k, "_topics.rda"))
if (!file.exists(filename)) {
m <- FitLdaModel(dtm = dtm, k = k, iterations = iter)
m$k <- k
m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
save(m, file = filename)
} else {
load(filename)
}
m
}, export=c("dtm", "model_dir")) # export only needed for Windows machines
#model tuning
#choosing the best model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)),
coherence = sapply(model_list, function(x) mean(x$coherence)),
stringsAsFactors = FALSE)
ggplot(coherence_mat, aes(x = k, y = coherence)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best Topic number by Coherence Score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,40,1)) + ylab("Coherence")
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1:2))  # key is to define vplayout
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1:2))  # key is to define vplayout
print(b, vp = vplayout(2, 1:2))
print(c, vp = vplayout(2, 1:2))
c <- ggplot(coherence_mat, aes(x = k, y = coherence)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best Topic number by Coherence Score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,40,1)) + ylab("Coherence")
b <- ggplot(perplexity, aes(x = k, y = p)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best topic number by perplexity score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,40,1)) + ylab("perplexity")
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
print(b, vp = vplayout(2, 1:2))
print(c, vp = vplayout(2, 1:2))
topic_number <- function(df){
set.seed(12345)
#matrix <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
## Find the best K Selection by Harmonic Mean. ; The harmonic mean function:
#harmonicMean <- function(logLikelihoods, precision = 2000L) {
#llMed <- median(logLikelihoods)
#as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
# prec = precision) + llMed))))
#}
##  We will use a sequence of numbers from 2 to 100, stepped by one. Using the lapply function, we run the LDA function using all the values of k. To see how much time is needed to run the process on your system, use the system.time function.
#seqk <- seq(2, 40, 1)
#burnin <- 4000 #1000
#iter <- 2000 #1000
#keep <- 100 #50
#system.time(fitted_many <- lapply(seqk,
#                                 function(k) topicmodels::LDA(matrix,
#                                                            k = k,
#                                                             method = "Gibbs",
#                                                             control = list(burnin = burnin,
#                                                                            iter = iter,
#                                                                            keep = keep) )))
# extract logliks from each topic
#logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
# compute harmonic means
#hm_total <- sapply(logLiks_many, function(h) harmonicMean(h))
#seqk[which.max(hm_total)]
#k <- seqk[which.max(hm_total)]
#plot(seqk, hm_total, type = "l")
#a <- ggplot(data.frame(seqk, hm_total), aes(x = seqk, y = hm_total)) + geom_path(lwd = 1.5) +
#  theme(text = element_text(family = NULL),
#     axis.title.y = element_text(vjust = 1, size = 16),
#      axis.title.x = element_text(vjust = -.5, size = 16),
#      axis.text = element_text(size = 16),
#     plot.title = element_text(size = 20)) +
#xlab('Number of Topics') +
#ylab('Harmonic Mean') +
#ylim(c(-10000000,-7500000)) +
#annotate("text", label = paste("The optimal number of topics is", seqk[which.max(hm_total)])) +
#ggtitle(expression(atop("Best Topic number by harmonicMean", atop(italic("How many distinct topics?"), "")))) +
#theme(axis.ticks = element_line(size = 1))+
#scale_x_continuous(breaks = seq(1, 50,by = 1))+
#theme_minimal()
## by perperxity
dtm <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
#Filtering empty rows of the matrix (DTM) - important for performance
rowTotals <- apply(dtm, 1, sum)
dtm_filter <- dtm[rowTotals > 0,]
#Model optimization - calculating perplexity for different values of k
set.seed(12345)
train = sample(rownames(dtm_filter), nrow(dtm_filter) * .75)
dtm_train = dtm_filter[rownames(dtm_filter) %in% train, ]
dtm_test = dtm_filter[!rownames(dtm_filter) %in% train, ]
perplexity = data.frame(k = 2:40, p=NA) #calculating perplexity for k 5:20
for (k in perplexity$k) {
message("k=", k)
m = LDA(dtm_train, method = "Gibbs", k = k,  control = list(alpha = 5/k))
perplexity$p[perplexity$k==k] = perplexity(m, dtm_test)
}
#perplexity
b <- ggplot(perplexity, aes(x = k, y = p)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best topic number by perplexity score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,40,1)) + ylab("perplexity")
#create DTM
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
k_list <- seq(1, 40, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
filename = file.path(model_dir, paste0(k, "_topics.rda"))
if (!file.exists(filename)) {
m <- FitLdaModel(dtm = dtm, k = k, iterations = iter)
m$k <- k
m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
save(m, file = filename)
} else {
load(filename)
}
m
}, export=c("dtm", "model_dir")) # export only needed for Windows machines
#model tuning
#choosing the best model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)),
coherence = sapply(model_list, function(x) mean(x$coherence)),
stringsAsFactors = FALSE)
c <- ggplot(coherence_mat, aes(x = k, y = coherence)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best Topic number by Coherence Score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,40,1)) + ylab("Coherence")
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
#print(a, vp = vplayout(1, 1:2))  # key is to define vplayout
print(b, vp = vplayout(1, 1:2))
print(c, vp = vplayout(2, 1:2))
}
topic_number(df)
dev.off()
topic_number <- function(df){
set.seed(12345)
#matrix <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
## Find the best K Selection by Harmonic Mean. ; The harmonic mean function:
#harmonicMean <- function(logLikelihoods, precision = 2000L) {
#llMed <- median(logLikelihoods)
#as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
# prec = precision) + llMed))))
#}
##  We will use a sequence of numbers from 2 to 100, stepped by one. Using the lapply function, we run the LDA function using all the values of k. To see how much time is needed to run the process on your system, use the system.time function.
#seqk <- seq(2, 30, 1)
#burnin <- 4000 #1000
#iter <- 2000 #1000
#keep <- 100 #50
#system.time(fitted_many <- lapply(seqk,
#                                 function(k) topicmodels::LDA(matrix,
#                                                            k = k,
#                                                             method = "Gibbs",
#                                                             control = list(burnin = burnin,
#                                                                            iter = iter,
#                                                                            keep = keep) )))
# extract logliks from each topic
#logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
# compute harmonic means
#hm_total <- sapply(logLiks_many, function(h) harmonicMean(h))
#seqk[which.max(hm_total)]
#k <- seqk[which.max(hm_total)]
#plot(seqk, hm_total, type = "l")
#a <- ggplot(data.frame(seqk, hm_total), aes(x = seqk, y = hm_total)) + geom_path(lwd = 1.5) +
#  theme(text = element_text(family = NULL),
#     axis.title.y = element_text(vjust = 1, size = 16),
#      axis.title.x = element_text(vjust = -.5, size = 16),
#      axis.text = element_text(size = 16),
#     plot.title = element_text(size = 20)) +
#xlab('Number of Topics') +
#ylab('Harmonic Mean') +
#ylim(c(-10000000,-7500000)) +
#annotate("text", label = paste("The optimal number of topics is", seqk[which.max(hm_total)])) +
#ggtitle(expression(atop("Best Topic number by harmonicMean", atop(italic("How many distinct topics?"), "")))) +
#theme(axis.ticks = element_line(size = 1))+
#scale_x_continuous(breaks = seq(1, 50,by = 1))+
#theme_minimal()
## by perperxity
dtm <- DocumentTermMatrix(Corpus(VectorSource(df$text)))
#Filtering empty rows of the matrix (DTM) - important for performance
rowTotals <- apply(dtm, 1, sum)
dtm_filter <- dtm[rowTotals > 0,]
#Model optimization - calculating perplexity for different values of k
set.seed(12345)
train = sample(rownames(dtm_filter), nrow(dtm_filter) * .75)
dtm_train = dtm_filter[rownames(dtm_filter) %in% train, ]
dtm_test = dtm_filter[!rownames(dtm_filter) %in% train, ]
perplexity = data.frame(k = 2:30, p=NA) #calculating perplexity for k 5:20
for (k in perplexity$k) {
message("k=", k)
m = LDA(dtm_train, method = "Gibbs", k = k,  control = list(alpha = 5/k))
perplexity$p[perplexity$k==k] = perplexity(m, dtm_test)
}
#perplexity
b <- ggplot(perplexity, aes(x = k, y = p)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best topic number by perplexity score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,30,1)) + ylab("perplexity")
#create DTM
dtm <- CreateDtm(df$text,
doc_names = df$status_id,
ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
k_list <- seq(1, 30, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
filename = file.path(model_dir, paste0(k, "_topics.rda"))
if (!file.exists(filename)) {
m <- FitLdaModel(dtm = dtm, k = k, iterations = iter)
m$k <- k
m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
save(m, file = filename)
} else {
load(filename)
}
m
}, export=c("dtm", "model_dir")) # export only needed for Windows machines
#model tuning
#choosing the best model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)),
coherence = sapply(model_list, function(x) mean(x$coherence)),
stringsAsFactors = FALSE)
c <- ggplot(coherence_mat, aes(x = k, y = coherence)) +
geom_point() +
geom_line(group = 1)+
ggtitle("Best Topic number by Coherence Score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,30,1)) + ylab("Coherence")
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
#print(a, vp = vplayout(1, 1:2))  # key is to define vplayout
print(b, vp = vplayout(1, 1:2))
print(c, vp = vplayout(2, 1:2))
}
topic_number(df)
load("F:/Bea/PostDoc/RESEARCH/ComTxt/data/NRC_Emotion_Lexicon.rda")
language = "es"
custom_lexicon <- NRC_Emotion_Lexicon %>% select(language, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
View(custom_lexicon)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
custom_lexicon <- NRC_Emotion_Lexicon %>% select(language, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
View(custom_lexicon)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
custom_lexicon <- custom_lexicon[rowSums(custom_lexicon[,2:9]) >0,]
undesirable_words <- "shit"
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3)
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3)
youtube_nrc <- nrc_tidy %>%
inner_join(custom_lexicon)
df_1 <- youtube_nrc[,1:91]
df_1<- df_1[df_1$Anger == 1,]
df_1$Anger <- "Anger"
colnames(df_1)[colnames(df_1) == "Anger"] <- "sentiment"
df_2 <- youtube_nrc[,c(1:90, 92)]
df_2<- df_2[df_2$Anticipation == 1,]
df_2$Anticipation <- "Anticipation"
colnames(df_2)[colnames(df_2) == "Anticipation"] <- "sentiment"
df_3 <- youtube_nrc[,c(1:90, 93)]
df_3<- df_3[df_3$Disgust == 1,]
df_3$Disgust <- "Disgust"
colnames(df_3)[colnames(df_3) == "Disgust"] <- "sentiment"
df_4 <- youtube_nrc[,c(1:90, 94)]
df_4<- df_4[df_4$Fear == 1,]
df_4$Fear <- "Fear"
colnames(df_4)[colnames(df_4) == "Fear"] <- "sentiment"
df_5 <- youtube_nrc[,c(1:90, 95)]
df_5<- df_5[df_5$Joy == 1,]
df_5$Joy <- "Joy"
colnames(df_5)[colnames(df_5) == "Joy"] <- "sentiment"
df_6 <- youtube_nrc[,c(1:90, 96)]
df_6<- df_6[df_6$Sadness == 1,]
df_6$Sadness <- "Sadness"
colnames(df_6)[colnames(df_6) == "Sadness"] <- "sentiment"
df_7 <- youtube_nrc[,c(1:90, 97)]
df_7<- df_7[df_7$Surprise == 1,]
df_7$Surprise <- "Surprise"
colnames(df_7)[colnames(df_7) == "Surprise"] <- "sentiment"
df_8 <- youtube_nrc[,c(1:90, 98)]
df_8<- df_8[df_8$Trust == 1,]
df_8$Trust <- "Trust"
colnames(df_8)[colnames(df_8) == "Trust"] <- "sentiment"
tmp <- rbind(df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8)
tmp <- tmp[order(tmp$created_at),]
twitter_sentiment <- function(df, language, undesirable_words){
custom_lexicon <- NRC_Emotion_Lexicon %>% select(language, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
custom_lexicon <- custom_lexicon[rowSums(custom_lexicon[,2:9]) >0,]
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3)
youtube_nrc <- nrc_tidy %>%
inner_join(custom_lexicon)
df_1 <- youtube_nrc[,1:91]
df_1<- df_1[df_1$Anger == 1,]
df_1$Anger <- "Anger"
colnames(df_1)[colnames(df_1) == "Anger"] <- "sentiment"
df_2 <- youtube_nrc[,c(1:90, 92)]
df_2<- df_2[df_2$Anticipation == 1,]
df_2$Anticipation <- "Anticipation"
colnames(df_2)[colnames(df_2) == "Anticipation"] <- "sentiment"
df_3 <- youtube_nrc[,c(1:90, 93)]
df_3<- df_3[df_3$Disgust == 1,]
df_3$Disgust <- "Disgust"
colnames(df_3)[colnames(df_3) == "Disgust"] <- "sentiment"
df_4 <- youtube_nrc[,c(1:90, 94)]
df_4<- df_4[df_4$Fear == 1,]
df_4$Fear <- "Fear"
colnames(df_4)[colnames(df_4) == "Fear"] <- "sentiment"
df_5 <- youtube_nrc[,c(1:90, 95)]
df_5<- df_5[df_5$Joy == 1,]
df_5$Joy <- "Joy"
colnames(df_5)[colnames(df_5) == "Joy"] <- "sentiment"
df_6 <- youtube_nrc[,c(1:90, 96)]
df_6<- df_6[df_6$Sadness == 1,]
df_6$Sadness <- "Sadness"
colnames(df_6)[colnames(df_6) == "Sadness"] <- "sentiment"
df_7 <- youtube_nrc[,c(1:90, 97)]
df_7<- df_7[df_7$Surprise == 1,]
df_7$Surprise <- "Surprise"
colnames(df_7)[colnames(df_7) == "Surprise"] <- "sentiment"
df_8 <- youtube_nrc[,c(1:90, 98)]
df_8<- df_8[df_8$Trust == 1,]
df_8$Trust <- "Trust"
colnames(df_8)[colnames(df_8) == "Trust"] <- "sentiment"
tmp <- rbind(df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8)
tmp <- tmp[order(tmp$created_at),]
return(tmp)
}
twitter_sentiment(df, language = "es", undesirable_words = "shit")
rm(NRC_Emotion_Lexicon)
rm(custom_lexicon)
load("F:/Bea/PostDoc/RESEARCH/ComTxt/data/NRC_Emotion_Lexicon.rda")
df_sent <- twitter_sentiment(df, language = "es", undesirable_words = "shit")
View(NRC_Emotion_Lexicon)
colnames(NRC_Emotion_Lexicon)[colnames(NRC_Emotion_Lexicon) == 'en...22'] <- 'en'
save(NRC_Emotion_Lexicon, file = "data/NRC_Emotion_Lexicon.rda")
