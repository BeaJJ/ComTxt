if(length(mentions_screen_name)==0){
mentions_screen_name <- NA
}  else {
mentions_screen_name <- mentions_screen_name
}
tmp.1[[i]] <- mentions_screen_name
}
tmp$mentions_screen_name <- tmp.1
## from user data
names(x$includes$users)[names(x$includes$users) == "id"] <- "author_id"
df <- left_join(x$data, x$includes$users, by = "author_id")
## follower count, following count, tweet count, listed_count
tmp$screen_name <-df$username
tmp <- cbind(tmp, df$public_metrics.y)
tmp$profile_image_url <-df$profile_image_url
tmp$name <-df$name
tmp$location <-df$location
tmp$description <-df$description
tmp$verified <-df$verified
tmp$profile_image_url <-df$profile_image_url
tmp$account_created_at <-df$created_at.y
tmp.2 <- list()
for (i in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[1]][[4]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.2[[i]] <- data.frame(profile_url)
}
tmp.2 <- do.call(rbind.data.frame, tmp.2)
tmp$profile_url <- tmp.2
names(x$includes$places)[names(x$includes$places) == "id"] <- "place_id"
x$includes$places <- subset(x$includes$places, select = -geo )
tmp <- left_join(tmp, x$includes$places, by = "place_id")
tmp
}
i
x <- list[[i]]
#tweets_list_clean <- lapply(list, function(x){
##from x$data
selec_col <- c("created_at","id","text","lang","author_id", "source","conversation_id")
tmp <- x$data[selec_col]
for(i in 1:nrow(x$data)){
if("in_reply_to_user_id" %in% colnames(x$data) == TRUE){
in_reply_to_user_id <- x$data$in_reply_to_user_id
}else{
in_reply_to_user_id <- NA}
}
tmp$in_reply_to_user_id <- in_reply_to_user_id
tmp$geo_coords <- x$data$geo$coordinates$coordinates
tmp$place_id <- x$data$geo$place_id
tmp$context_annotations <- x$data$context_annotations
tmp <- cbind(tmp, x$data$public_metrics)
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_t.co <- x$data$entities$urls[[i]][[3]]
if(length(url_t.co)==0){
url_t.co <- NA
} else {
url_t.co <- url_t.co
}
tmp.1[[i]] <- url_t.co
}
tmp$url_t.co <- tmp.1
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_extended_url<- x$data$entities$urls[[i]][[4]]
if(length(url_extended_url)==0){
url_extended_url <- NA
} else {
url_extended_url <- url_extended_url
}
tmp.1[[i]] <- url_extended_url
}
x$data$entities$urls[[1]]
x$data$entities$urls
View(x$data$entities$urls)
x$data$entities$urls[[i]][[3]]
i
i= 1
x$data$entities$urls[[i]][[3]]
url_t.co <- x$data$entities$urls[[i]][[3]]
length(url_t.co)
tweets_list_clean <- lapply(list, function(x){
##from x$data
selec_col <- c("created_at","id","text","lang","author_id", "source","conversation_id")
tmp <- x$data[selec_col]
for(i in 1:nrow(x$data)){
if("in_reply_to_user_id" %in% colnames(x$data) == TRUE){
in_reply_to_user_id <- x$data$in_reply_to_user_id
}else{
in_reply_to_user_id <- NA}
}
tmp$in_reply_to_user_id <- in_reply_to_user_id
tmp$geo_coords <- x$data$geo$coordinates$coordinates
tmp$place_id <- x$data$geo$place_id
tmp$context_annotations <- x$data$context_annotations
tmp <- cbind(tmp, x$data$public_metrics)
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_t.co <- x$data$entities$urls[[i]][[3]]
if(length(url_t.co)==0){
url_t.co <- NA
} else {
url_t.co <- url_t.co
}
tmp.1[[i]] <- url_t.co
}
tmp$url_t.co <- tmp.1
tmp.2 <- list()
for (j in 1:nrow(x$data)){
url_extended_url<- x$data$entities$urls[[j]][[4]]
if(length(url_extended_url)==0){
url_extended_url <- NA
} else {
url_extended_url <- url_extended_url
}
tmp.2[[j]] <- url_extended_url
}
tmp$url_extended_url <- tmp.2
tmp.3 <- list()
for (k in 1:nrow(x$data)){
hashtags <- x$data$entities$hashtags[[k]][[3]]
if(length(hashtags) == 0){
hashtags <- NA
} else{
hashtags <- hashtags
}
tmp.3[[k]] <- hashtags
}
tmp$hashtags <- tmp.3
tmp.4 <- list()
for (n in 1:nrow(x$data)){
mentions_screen_name <- x$data$entities$mentions[[n]][[3]]
if(length(mentions_screen_name)==0){
mentions_screen_name <- NA
}  else {
mentions_screen_name <- mentions_screen_name
}
tmp.4[[n]] <- mentions_screen_name
}
tmp$mentions_screen_name <- tmp.4
## from user data
names(x$includes$users)[names(x$includes$users) == "id"] <- "author_id"
df <- left_join(x$data, x$includes$users, by = "author_id")
## follower count, following count, tweet count, listed_count
tmp$screen_name <-df$username
tmp <- cbind(tmp, df$public_metrics.y)
tmp$profile_image_url <-df$profile_image_url
tmp$name <-df$name
tmp$location <-df$location
tmp$description <-df$description
tmp$verified <-df$verified
tmp$profile_image_url <-df$profile_image_url
tmp$account_created_at <-df$created_at.y
tmp.5 <- list()
for (m in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[m]][[4]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.5[[m]] <- data.frame(profile_url)
}
tmp.5 <- do.call(rbind.data.frame, tmp.5)
tmp$profile_url <- tmp.5
names(x$includes$places)[names(x$includes$places) == "id"] <- "place_id"
x$includes$places <- subset(x$includes$places, select = -geo )
tmp <- left_join(tmp, x$includes$places, by = "place_id")
return(tmp)
})
tmp.5 <- list()
for (m in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[m]][[4]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.5[[m]] <- data.frame(profile_url)
}
m
df
View(df)
length(df$entities.y$url$urls)
df$entities.y$url$urls[[m]][[4]]
df$entities.y$url$urls[[1]][[4]]
df$entities.y$url$urls
View(df$entities.y$url$urls)
df$entities.y$url$urls[[1]][["url"]]
df$entities.y$url$urls[[2]][["url"]]
df$entities.y$url$urls[[3]][["url"]]
tweets_list_clean <- lapply(list, function(x){
##from x$data
selec_col <- c("created_at","id","text","lang","author_id", "source","conversation_id")
tmp <- x$data[selec_col]
for(i in 1:nrow(x$data)){
if("in_reply_to_user_id" %in% colnames(x$data) == TRUE){
in_reply_to_user_id <- x$data$in_reply_to_user_id
}else{
in_reply_to_user_id <- NA}
}
tmp$in_reply_to_user_id <- in_reply_to_user_id
tmp$geo_coords <- x$data$geo$coordinates$coordinates
tmp$place_id <- x$data$geo$place_id
tmp$context_annotations <- x$data$context_annotations
tmp <- cbind(tmp, x$data$public_metrics)
tmp.1 <- list()
for (i in 1:nrow(x$data)){
url_t.co <- x$data$entities$urls[[i]][[3]]
if(length(url_t.co)==0){
url_t.co <- NA
} else {
url_t.co <- url_t.co
}
tmp.1[[i]] <- url_t.co
}
tmp$url_t.co <- tmp.1
tmp.2 <- list()
for (j in 1:nrow(x$data)){
url_extended_url<- x$data$entities$urls[[j]][[4]]
if(length(url_extended_url)==0){
url_extended_url <- NA
} else {
url_extended_url <- url_extended_url
}
tmp.2[[j]] <- url_extended_url
}
tmp$url_extended_url <- tmp.2
tmp.3 <- list()
for (k in 1:nrow(x$data)){
hashtags <- x$data$entities$hashtags[[k]][[3]]
if(length(hashtags) == 0){
hashtags <- NA
} else{
hashtags <- hashtags
}
tmp.3[[k]] <- hashtags
}
tmp$hashtags <- tmp.3
tmp.4 <- list()
for (n in 1:nrow(x$data)){
mentions_screen_name <- x$data$entities$mentions[[n]][[3]]
if(length(mentions_screen_name)==0){
mentions_screen_name <- NA
}  else {
mentions_screen_name <- mentions_screen_name
}
tmp.4[[n]] <- mentions_screen_name
}
tmp$mentions_screen_name <- tmp.4
## from user data
names(x$includes$users)[names(x$includes$users) == "id"] <- "author_id"
df <- left_join(x$data, x$includes$users, by = "author_id")
## follower count, following count, tweet count, listed_count
tmp$screen_name <-df$username
tmp <- cbind(tmp, df$public_metrics.y)
tmp$profile_image_url <-df$profile_image_url
tmp$name <-df$name
tmp$location <-df$location
tmp$description <-df$description
tmp$verified <-df$verified
tmp$profile_image_url <-df$profile_image_url
tmp$account_created_at <-df$created_at.y
tmp.5 <- list()
for (m in 1:length(df$entities.y$url$urls)){
profile_url <- df$entities.y$url$urls[[m]][["url"]]
if(length(profile_url)==0){
profile_url <- NA
} else {
profile_url <- profile_url
}
tmp.5[[m]] <- data.frame(profile_url)
}
tmp.5 <- do.call(rbind.data.frame, tmp.5)
tmp$profile_url <- tmp.5
names(x$includes$places)[names(x$includes$places) == "id"] <- "place_id"
x$includes$places <- subset(x$includes$places, select = -geo )
tmp <- left_join(tmp, x$includes$places, by = "place_id")
return(tmp)
})
x$data$entities$urls[[i]]
x$data$entities$urls[[2]]
x$data$entities$urls[[2]][["url"]]
x$data$entities$urls[[2]][["expanded_url"]]
x$data$entities$hashtags[[1]]
x$data$entities$hashtags[[2]]
x$data$entities$mentions[[1]]
x$data$entities$mentions[[1]][["username"]]
x$data$entities$mentions[[1]][[3]]
library(ComTxt)
library(ComTxt)
geo_preprocess <- function(df, location_country,  multi_name){
## Seperating city and country with ,
tmp <- list()
for(i in 1:length(df$location)){
tmp[[i]] <- unlist(strsplit(as.character(df$location[i]), ","))
}
location_1 <- list()
for(i in 1:length(tmp)){
location_1[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_1 <- reduce(location_1, bind_rows)
## location_tmp: seperating cities and country ()
tmp <- list()
for(i in 1:nrow(location_1)){
tmp[[i]] <- unlist(strsplit(as.character(location_1[i, 1]), "\\(|\\)"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
## location_ df: combining location_1 data and location_tmp data adjusting contry coulmn
location_df <- cbind(location_tmp, location_1[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
## location_tmp: seperating city and country with -
tmp <- list()
for(i in 1:nrow(location_df)){
tmp[[i]] <- unlist(strsplit(as.character(location_df[i, 1]), "[-]"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
## location_df: combining loctaion_df and location tmp
location_df <- cbind(location_tmp, location_df[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
##location_tmp: seperating city and country with _
tmp <- list()
for(i in 1:nrow(location_df)){
tmp[[i]] <- unlist(strsplit(as.character(location_df[i, 1]), "[_]"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <- list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
## location_df: combining loctaion_df and location tmp
location_df <- cbind(location_tmp, location_df[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
## location_tmp : seperating cities "[.]"
tmp <- list()
for(i in 1:nrow(location_df)){
tmp[[i]] <- unlist(strsplit(as.character(location_df[i, 1]), "[.]"))
}
location_tmp <- list()
for(i in 1:length(tmp)){
location_tmp[[i]] <-list(city = unlist(tmp[i])[1], country = unlist(tmp[i])[2])
}
location_tmp <- reduce(location_tmp, bind_rows)
##location_df :combining loctaion_df and location tmp
location_df <- cbind(location_tmp, location_df[,2])
for(i in 1:nrow(location_df)){
if (is.na(location_df[[i,3]]) == FALSE){
location_df[[i,2]] <- as.character(location_df[[i,3]])
}
}
location_df <- location_df[,1:2]
## cleaning text -------
location_df[,1] <- gsub("\\|..*", "", location_df[,1])##deleting multiple cities
location_df[,1] <- gsub("<u+.*", "", location_df[,1])##deleting unicode
location_df[,1] <- gsub("[0-9]+", "",location_df[,1], perl = T)
location_df[,2] <- gsub("[0-9]+", "",location_df[,2], perl = T)
location_df[,1] <- gsub("[[:punct:][:blank:]]+", " ", location_df[,1], perl = T)##deleting puntuation city
location_df[,2] <- gsub("[[:punct:][:blank:]]+", " ", location_df[,2], perl = T)##deleting puntuation country
location_df$country <- trimws(location_df$country)
##language specific
location_df[,2] <- tolower(location_df[,2])##lowercase
location_df[,1] <- tolower(location_df[,1])##lowercase
location_df <-
location_df %>%
mutate(
country = ifelse(location_df$city %in% multi_name, location_country, location_df$country)
)
location_df <-
location_df %>%
mutate(
country = ifelse(location_df$country %in% multi_name, location_country, location_df$country)
)
## if cities names is spain add to country column----
location_df <- as.data.frame(location_df)
location_df <-
location_df %>%
mutate(
country =ifelse(location_df[,1] == location_country, location_country, location_df[,2])
)
location_df[,1] <- gsub("^\\s+|\\s+$", "", location_df[,1])##deleting space
location_df[,2] <- gsub("^\\s+|\\s+$", "", location_df[,2])##deleting space
##geocode selecting
#geo <- list()
#for(i in 1:length(df$location)){
# if(is.na(df$geo_coords[[i]][[1]])){
#    geo[[i]] <- list(lat = NA, lng = NA)
#  } else {
#   geo[[i]] <- list(lat = df$geo_coords[[i]][[1]], lng = df$geo_coords[[i]][[2]], country = location_country)
# }
#}
#geo <- reduce(geo, bind_rows)
##location_df :combining loctaion_df and location tmp
#location_df <- cbind(location_df, geo[,3])
#for(i in 1:nrow(location_df)){
#  if (is.na(location_df[[i,3]]) == FALSE){
#   location_df[[i,2]] <- as.character(location_df[[i,3]])
# }
#}
location_df <- location_df[,1:2]
data("world.cities")
## now lets find the cities cities
world.cities$country.etc <- tolower(world.cities$country.etc)
city_df <- world.cities[world.cities$country.etc == location_country, ]
city_df$name <- tolower(city_df$name)
location_df <-
location_df %>%
mutate(
country = ifelse(location_df$city %in% city_df$name, location_country, location_df$country)
)
## add to text_clean data fram
df <- subset(df, select = -c(location, country))
df <- data.frame(df, location_df)
## add lat lang from geo_coord
#df <- cbind(df, geo[,1:2])
city_df <- world.cities[world.cities$country.etc == location_country, ]
city_df$name <- tolower(city_df$name)
#geom <- list()
for(i in 1:nrow(df)){
if(nrow(city_df[city_df$name == df$city[i],])==1){
df$lat[[i]] <- city_df[city_df$name == df$city[i],]$lat
df$lng[[i]] <- city_df[city_df$name == df$city[i],]$long
}
if(nrow(city_df[city_df$name == df$city[i],])==0) {
df$lat[[i]] <- NA
df$lng[[i]] <- NA
}
}
df <- df[which(df$country == location_country),]
for(i in 1:nrow(df)){
if(is.na(df$lat[i])==TRUE){
if(is.na(df$geo_coords[[i]][[2]])==TRUE){
df$lat[[i]] <- NA
df$lng[[i]] <-NA
}else{
df$lat[[i]] <- df$geo_coords[[i]][[1]]
df_tweets$lng[[i]] <- df$geo_coords[[i]][[2]]
}
}else{
df$lat[[i]] <- df$lat[[i]]
df$lng[[i]] <- df$lng[[i]]
}
}
return(df)
}
hashtags_network <- function(df, hashtag_remove = "#cultura", top_n = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df$hashtags <- iconv(df$hashtags,from="UTF-8",to="ASCII//TRANSLIT")
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags)
hash_dfm <- dfm_remove(hash_dfm, hashtag_remove)# Document-feature matrix
toptag <- names(topfeatures(hash_dfm, top_n)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
##draw semantic network plot
quanteda.textplots::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
hashtags_network_hub <- function(df, hub, top_n = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df$hashtags <- iconv(df$hashtags,from="UTF-8",to="ASCII//TRANSLIT")
df <- df[!is.na(df$hashtags),]
hash_dfm <- tokens(df$hashtags) # Document-feature matrix
fcmat <- fcm(hash_dfm , context = "window", tri = FALSE) # Feature-occurance matrix which shows the network structure
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
nodes_degree <- tmp[tmp$nodes == hub, ]$degree
high_nodes <- tmp[tmp$degree > nodes_degree, ]$nodes
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
fcm_local <- fcm_remove(fcm_local, high_nodes)
##reduce only top words
feat <-  names(topfeatures(fcm_local, top_n))
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
##text plot
quanteda.textplots::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
}
library(ComTxt)
library(ComTxt)
