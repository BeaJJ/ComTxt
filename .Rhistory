grid.col<- c(unique(d$month)= my_colors[i] )
grid.col<- c(data = my_colors[i] )
date <- unique(d$month)[[i]]
grid.col<- c(date = my_colors[i] )
grid.col
unique(d$month)
grid.col<- c("02/2021" = my_colors[i] )
grid.col<- c(unique(d$month) = my_colors[i] )
grid.col<- c(unique(d$month) == my_colors[i] )
grid.col
grid.col<- c(unique(d$month) = my_colors[i] )
class(grid.col)
grid.col<- c("02/2021" = my_colors[i] )
class(grid.col)
grid.col
## circos map -----------------------------------------------------
circos_map <- function(df, by){
df$created_at <- format(df$created_at, "%Y")
my_colors <- c(brewer.pal(8, "Accent"), brewer.pal(8, "Dark2"), brewer.pal(12, "Paired"))
#grid.col = c("02/2021" = my_colors[1], "01/2021" = my_colors[2], "12/2020" = my_colors[3], "11/2020" = my_colors[4], "10/2020" = my_colors[5], "9/2020" = my_colors[6],"8/2020" = my_colors[7],"7/2020" = my_colors[8],"6/2020" = my_colors[9],"5/2020" = my_colors[10],"4/2020" = my_colors[11],"3/2020" = my_colors[12], "2/2020" = my_colors[13],"1/2020" = my_colors[14],"12/2019" = my_colors[15],"11/2019" = my_colors[16],"10/2019" = my_colors[17],"9/2019" = my_colors[18],"8/2019" = my_colors[19],"7/2019" = my_colors[20],"6/2019" = my_colors[21],"5/2019" = my_colors[22],"4/2019" = my_colors[23],"3/2019" = my_colors[24],"2/2019" = my_colors[25],"1/2019" = my_colors[26],"anger" = "grey", "anticipation" = "grey", "disgust" = "grey", "fear" = "grey", "joy" = "grey", "sadness" = "grey", "surprise" = "grey", "trust" = "grey")
my_colors <- c(brewer.pal(8, "Accent"), brewer.pal(8, "Dark2"), brewer.pal(12, "Paired"))
grid.col = c("2021" = my_colors[1], "2020" = my_colors[2], "2019" = my_colors[3], "anger" = "grey", "anticipation" = "grey", "disgust" = "grey", "fear" = "grey", "joy" = "grey", "sadness" = "grey", "surprise" = "grey", "trust" = "grey")
decade_mood <-  df %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(sentiment, created_at) %>%
group_by(created_at, sentiment) %>%
summarise(sentiment_sum = sum(n)) %>%
ungroup()
circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(decade_mood[[1]])) - 1), 15,
rep(5, length(unique(decade_mood[[2]])) - 1), 15))
chordDiagram(decade_mood, grid.col = grid.col, transparency = .2,annotationTrack = c("grid", "axis"))
circos.track(track.index = 1, panel.fun = function(x, y) {
circos.text(CELL_META$xcenter, CELL_META$ylim[1], CELL_META$sector.index,
facing = "clockwise", niceFacing = TRUE, adj = c(0, 0.3))
}, bg.border = NA)
title("Relationship Between Mood and Years")
}
circos_map <- function(df){
df$created_at <- format(df$created_at, "%Y")
my_colors <- c(brewer.pal(8, "Accent"), brewer.pal(8, "Dark2"), brewer.pal(12, "Paired"))
#grid.col = c("02/2021" = my_colors[1], "01/2021" = my_colors[2], "12/2020" = my_colors[3], "11/2020" = my_colors[4], "10/2020" = my_colors[5], "9/2020" = my_colors[6],"8/2020" = my_colors[7],"7/2020" = my_colors[8],"6/2020" = my_colors[9],"5/2020" = my_colors[10],"4/2020" = my_colors[11],"3/2020" = my_colors[12], "2/2020" = my_colors[13],"1/2020" = my_colors[14],"12/2019" = my_colors[15],"11/2019" = my_colors[16],"10/2019" = my_colors[17],"9/2019" = my_colors[18],"8/2019" = my_colors[19],"7/2019" = my_colors[20],"6/2019" = my_colors[21],"5/2019" = my_colors[22],"4/2019" = my_colors[23],"3/2019" = my_colors[24],"2/2019" = my_colors[25],"1/2019" = my_colors[26],"anger" = "grey", "anticipation" = "grey", "disgust" = "grey", "fear" = "grey", "joy" = "grey", "sadness" = "grey", "surprise" = "grey", "trust" = "grey")
my_colors <- c(brewer.pal(8, "Accent"), brewer.pal(8, "Dark2"), brewer.pal(12, "Paired"))
grid.col = c("2021" = my_colors[1], "2020" = my_colors[2], "2019" = my_colors[3], "anger" = "grey", "anticipation" = "grey", "disgust" = "grey", "fear" = "grey", "joy" = "grey", "sadness" = "grey", "surprise" = "grey", "trust" = "grey")
decade_mood <-  df %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(sentiment, created_at) %>%
group_by(created_at, sentiment) %>%
summarise(sentiment_sum = sum(n)) %>%
ungroup()
circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(decade_mood[[1]])) - 1), 15,
rep(5, length(unique(decade_mood[[2]])) - 1), 15))
chordDiagram(decade_mood, grid.col = grid.col, transparency = .2,annotationTrack = c("grid", "axis"))
circos.track(track.index = 1, panel.fun = function(x, y) {
circos.text(CELL_META$xcenter, CELL_META$ylim[1], CELL_META$sector.index,
facing = "clockwise", niceFacing = TRUE, adj = c(0, 0.3))
}, bg.border = NA)
title("Relationship Between Mood and Years")
}
circos_map(d)
## each words used in each video---------------------------------------
plot_words <- function(df){
plot_words_UnderstandingLOU2 <- df %>%
group_by(sentiment) %>%
count(word, sort = TRUE) %>%
arrange(desc(n)) %>%
slice(seq_len(10)) %>%
ungroup()
plot_words_UnderstandingLOU2 %>%
ggplot(aes(word, 1, label = word, fill = sentiment )) +
geom_point(color = "transparent") +
geom_label_repel(force = 1,nudge_y = .5,
direction = "y",
box.padding = 0.05,
segment.color = "transparent",
size = 3) +
facet_grid(~sentiment) +
theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
axis.title.x = element_text(size = 6),
panel.grid = element_blank(), panel.background = element_blank(),
panel.border = element_rect("lightgray", fill = NA),
strip.text.x = element_text(size = 9)) +
xlab(NULL) + ylab(NULL) +
ggtitle("NRC Sentiment words plot") +
coord_flip()
}
plot_words(d)
## each words used in each video---------------------------------------
plot_words <- function(df){
plot_words_UnderstandingLOU2 <- df %>%
group_by(sentiment) %>%
count(word, sort = TRUE) %>%
arrange(desc(n)) %>%
slice(seq_len(10)) %>%
ungroup()
plot_words_UnderstandingLOU2 %>%
ggplot(aes(word, 1, label = word, fill = sentiment )) +
geom_point(color = "transparent") +
geom_label_repel(force = 1,nudge_y = .5,
direction = "y",
box.padding = 0.05,
segment.color = "transparent",
size = 3) +
facet_grid(~sentiment) +
theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
axis.title.x = element_text(size = 6),
panel.grid = element_blank(), panel.background = element_blank(),
panel.border = element_rect("lightgray", fill = NA),
strip.text.x = element_text(size = 9)) +
xlab(NULL) + ylab(NULL) +
ggtitle("NRC Sentiment words plot") +
coord_flip()+
theme(legend.position = "none")
}
plot_words(d)
## sentiment radarmap ------------------------------
radar_map <- function(){
df$created_at <- format(df$created_at, "%Y")
prince_nrc_sub <- df %>%
filter(!sentiment %in% c("positive", "negative"))
year_sentiment_nrc <- prince_nrc_sub %>%
group_by(created_at, sentiment) %>%
count(created_at, sentiment) %>%
select(created_at, sentiment, sentiment_year_count = n)
#Get the total count of sentiment words per video_name (not distinct)
total_sentiment_video <- prince_nrc_sub %>%
count(created_at) %>%
select(created_at, year_total = n)
col <- col2rgb(c("peachpuff", "royalblue", "tomato", "#B4CF68", "green", "purple", "orange","grey","red","#8DD3C7" , "pink", "blue","gold"))
#Join the two and create a percent field
year_radar_chart <- year_sentiment_nrc %>%
inner_join(total_sentiment_video, by = "created_at") %>%
mutate(percent = sentiment_year_count / year_total * 100 ) %>%
#filter(year %in% c("1978","1994","1995")) %>%
select(-sentiment_year_count, -year_total) %>%
spread(created_at, percent) %>%
chartJSRadar(showToolTipLabel = TRUE,
main = "NRC video Radar", colMatrix =col)
print(year_radar_chart)
}
radar_map(d)
## sentiment radarmap ------------------------------
radar_map <- function(df){
df$created_at <- format(df$created_at, "%Y")
prince_nrc_sub <- df %>%
filter(!sentiment %in% c("positive", "negative"))
year_sentiment_nrc <- prince_nrc_sub %>%
group_by(created_at, sentiment) %>%
count(created_at, sentiment) %>%
select(created_at, sentiment, sentiment_year_count = n)
#Get the total count of sentiment words per video_name (not distinct)
total_sentiment_video <- prince_nrc_sub %>%
count(created_at) %>%
select(created_at, year_total = n)
col <- col2rgb(c("peachpuff", "royalblue", "tomato", "#B4CF68", "green", "purple", "orange","grey","red","#8DD3C7" , "pink", "blue","gold"))
#Join the two and create a percent field
year_radar_chart <- year_sentiment_nrc %>%
inner_join(total_sentiment_video, by = "created_at") %>%
mutate(percent = sentiment_year_count / year_total * 100 ) %>%
#filter(year %in% c("1978","1994","1995")) %>%
select(-sentiment_year_count, -year_total) %>%
spread(created_at, percent) %>%
chartJSRadar(showToolTipLabel = TRUE,
main = "NRC video Radar", colMatrix =col)
print(year_radar_chart)
}
radar_map(d)
## Semantic network for anger and fear sadness---------------------
sentiment_semantic <- function(df, select = "anger"){
select_nrc <- df %>%
filter(sentiment %in% select)
user_data <- unique(select_nrc$authorDisplayName)
net <- list()
for (i in 1:length(user_data)){
if(length(select_nrc[select_nrc$authorDisplayName == user_data[i],]$word)>1){
word <- paste(unique(select_nrc[select_nrc$authorDisplayName == user_data[i],]$word), sep = "", collapse = " ")
} else {
word<- select_nrc[select_nrc$authorDisplayName == user_data[i],]$word
}
net[[i]] <- as.data.frame(cbind(id = user_data[i], word))
}
net_anger <- do.call(rbind, net)
library(quanteda)
library(quanteda.textplots)
tweets_dfm <- dfm(net_anger$word) # Document-feature matrix
#tag_dfm <- dfm_select(tweets_dfm, pattern = ("#*")) # Get hashtags
toptag <- names(topfeatures(tweets_dfm, 100)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(tweets_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
textplot_network(topgat_fcm, min_freq = 12, edge_alpha = 0.5, edge_size = 5, edge_color = "grey",vertex_color ="#538797")
}
sentiment_semantic(d, select = "anger")
select_nrc <- d %>%
filter(sentiment %in% "surprize")
sentiment_semantic(d, select = "surprise")
select_nrc <- d %>%
filter(sentiment %in% "surprize")
View(select_nrc)
View(d)
select_nrc <- d %>%
filter(sentiment %in% "surprise")
View(select_nrc)
sentiment_semantic <- function(df, select = "surprise"){
select_nrc <- df %>%
filter(sentiment %in% select)
user_data <- unique(select_nrc$status_id)
net <- list()
for (i in 1:length(user_data)){
if(length(select_nrc[select_nrc$status_id == user_data[i],]$word)>1){
word <- paste(unique(select_nrc[select_nrc$status_id == user_data[i],]$word), sep = "", collapse = " ")
} else {
word<- select_nrc[select_nrc$status_id == user_data[i],]$word
}
net[[i]] <- as.data.frame(cbind(id = user_data[i], word))
}
net_anger <- do.call(rbind, net)
tweets_dfm <- dfm(net_anger$word) # Document-feature matrix
#tag_dfm <- dfm_select(tweets_dfm, pattern = ("#*")) # Get hashtags
toptag <- names(topfeatures(tweets_dfm, 100)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(tweets_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
textplot_network(topgat_fcm, min_freq = 12, edge_alpha = 0.5, edge_size = 5, edge_color = "grey",vertex_color ="#538797")
}
sentiment_semantic(d, select = "surprise")
## Semantic network for anger and fear sadness---------------------
library(quanteda)
library(quanteda.textplots)
sentiment_semantic(d, select = "surprise")
select_nrc <- d %>%
filter(sentiment %in% "surprise")
user_data <- unique(select_nrc$status_id)
net <- list()
for (i in 1:length(user_data)){
if(length(select_nrc[select_nrc$status_id == user_data[i],]$word)>1){
word <- paste(unique(select_nrc[select_nrc$status_id == user_data[i],]$word), sep = "", collapse = " ")
} else {
word<- select_nrc[select_nrc$status_id == user_data[i],]$word
}
net[[i]] <- as.data.frame(cbind(id = user_data[i], word))
}
net_anger <- do.call(rbind, net)
tweets_dfm <- dfm(net_anger$word) # Document-feature matrix
#tag_dfm <- dfm_select(tweets_dfm, pattern = ("#*")) # Get hashtags
toptag <- names(topfeatures(tweets_dfm, 100)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(tweets_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
textplot_network(topgat_fcm, min_freq = 12, edge_alpha = 0.5, edge_size = 5, edge_color = "grey",vertex_color ="#538797")
textplot_network(topgat_fcm, min_freq = 1, edge_alpha = 0.5, edge_size = 5, edge_color = "grey",vertex_color ="#538797")
textplot_network(topgat_fcm,  edge_alpha = 0.5, edge_size = 5, edge_color = "grey",vertex_color ="#538797")
sentiment_semantic <- function(df, select = "surprise", n_nodes = 100){
select_nrc <- df %>%
filter(sentiment %in% select)
user_data <- unique(select_nrc$status_id)
net <- list()
for (i in 1:length(user_data)){
if(length(select_nrc[select_nrc$status_id == user_data[i],]$word)>1){
word <- paste(unique(select_nrc[select_nrc$status_id == user_data[i],]$word), sep = "", collapse = " ")
} else {
word<- select_nrc[select_nrc$status_id == user_data[i],]$word
}
net[[i]] <- as.data.frame(cbind(id = user_data[i], word))
}
net_anger <- do.call(rbind, net)
tweets_dfm <- dfm(net_anger$word) # Document-feature matrix
#tag_dfm <- dfm_select(tweets_dfm, pattern = ("#*")) # Get hashtags
toptag <- names(topfeatures(tweets_dfm, n_nodes)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(tweets_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
textplot_network(topgat_fcm,  edge_alpha = 0.5, edge_size = 5, edge_color = "grey",vertex_color ="#538797")
}
sentiment_semantic(d, select = "surprise")
sentiment_semantic(d, select = "surprise", n_nodes = 200)
sentiment_semantic(d, select = "anger", n_nodes = 200)
sentiment_semantic(d, select = "joy", n_nodes = 200)
sentiment_semantic(d, select = "anticipation", n_nodes = 200)
basic_tweets <- readRDS("C:/Users/LocalAdmin/OneDrive - Universitat Autònoma de Barcelona/ComTxt/source/basic_tweets.Rds")
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
twitter_sentiment <- function(df, language, undesirable_words){
stopwords <- data.frame(word = stopwords::stopwords(language = language, source = "stopwords-iso"))
stopwords$lexicon <- language
stopwords <- as_tibble(stopwords)
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3) %>% #Words like "ah" or "oo" used in music
anti_join(stopwords)
youtube_nrc <- nrc_tidy %>%
inner_join(get_sentiments("nrc"))
}
d <- twitter_sentiment(basic_tweets, language = "es", undesirable_words = c("shit"))
View(d)
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation
library(textdata)
twitter_sentiment <- function(df, language, undesirable_words){
stopwords <- data.frame(word = stopwords::stopwords(language = language, source = "stopwords-iso"))
stopwords$lexicon <- language
stopwords <- as_tibble(stopwords)
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3) %>% #Words like "ah" or "oo" used in music
anti_join(stopwords)
youtube_nrc <- nrc_tidy %>%
inner_join(get_sentiments("nrc"))
}
library(xlsx)
nrc_lexicon <- read.xlsx("source/NRC-Emoticon-Lexicon-v0.92-In105Languages-Now2017Translation.xlsx")
nrc_lexicon <- read.xlsx("source/NRC-Emoticon-Lexicon-v0.92-In105Languages-Now2017Translation.xlsx",sheetIndex = 1)
nrc_lexicon <- read.xlsx("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx",sheetIndex = 1)
library(readxl)
NRC_Emotion_Lexicon_v0_92_In105Languages_Nov2017Translations <- read_excel("C:/Users/LocalAdmin/OneDrive - Universitat Autònoma de Barcelona/ComTxt/source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
View(NRC_Emotion_Lexicon_v0_92_In105Languages_Nov2017Translations)
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon_v0_92_In105Languages_Nov2017Translations
rm(NRC_Emotion_Lexicon_v0_92_In105Languages_Nov2017Translations)
class(NRC_Emotion_Lexicon)
NRC_Emotion_Lexicon %>% select(Spanish (es), Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
NRC_Emotion_Lexicon %>% select(Spanish, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
NRC_Emotion_Lexicon %>% select('Spanish (es)', Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
View(NRC_Emotion_Lexicon)
langauge <- 'Spanish (es)'
NRC_Emotion_Lexicon %>% select(langauge, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(NRC_Emotion_Lexicon)
names <- colnames(NRC_Emotion_Lexicon)
gsub(" (*.*)", " ",names)
gsub(" (*.*)", "",names)
colnames(NRC_Emotion_Lexicon) <- gsub(" (*.*)", " ",names)
View(NRC_Emotion_Lexicon)
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(-1)
View(NRC_Emotion_Lexicon)
View(NRC_Emotion_Lexicon)
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select( - 1)
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(2:115)
NRC_Emotion_Lexicon <- read_excel("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
names <- colnames(NRC_Emotion_Lexicon)
colnames(NRC_Emotion_Lexicon)
name <- colnames(NRC_Emotion_Lexicon)
gsub( " *\\(.*?\\) *", "(.*?\\)", name)
gsub( " *\\ (", " ", name)
gsub( " *\\*(", " ", name)
gsub( " *\\..(", " ", name)
gsub( " *\\..", " ", name)
gsub( " *\\() *", "(.*?\\)", name)
gsub( " *\\()*", "(.*?\\)", name)
gsub( " *\\(", " ", name)
gsub( " *\\(*", " ", name)
gsub( " *\\(*", "", name)
gsub( " *\\(\\*", " ", name)
gsub( " *\\(\\..*", " ", name)
gsub( " *\\..(..*", " ", name)
gsub( " *\\..( *", " ", name)
gsub( " *\\( *", " ", name)
gsub( "*\\( *", " ", name)
gsub( "*\\(*", " ", name)
gsub( "*\\(*\\", " ", name)
gsub( "*\\(\\*", " ", name)
gsub( "*\\..(\\*", " ", name)
gsub( "*\\*(\\*", " ", name)
gsub( "*\\+*(\\*", " ", name)
gsub( "*(*", " ", name)
gsub( "*\\*(", " ", name)
gsub( "*\\(", " ", name)
gsub( "**\\(", " ", name)
gsub( "*(", " ", name)
gsub( ".*(", " ", name)
gsub( ".*.(", " ", name)
gsub( ".*.( ", " ", name)
gsub( ".*.( *", " ", name)
gsub( ".*?(\\)", " ", name)
gsub( ".*?\\", " ", name)
gsub( ".*?\\ (", " ", name)
gsub( ".*. (", " ", name)
gsub( "*. (", " ", name)
gsub( "*. ()", " ", name)
gsub( "*.\\()", " ", name)
gsub( "*.\\() *", " ", name)
gsub( "*.\\*() *", " ", name)
gsub(".*(:)","",foo)
gsub(".*(","",foo)
gsub(".* (","",foo)
gsub(".* ","",foo)
gsub(".* ","",name)
name <- gsub(".* ","",name)
name <- gsub("(","",name)
gsub("[()]","",name)
NRC_Emotion_Lexicon <- read_excel("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(-("English (en)...1"))
names <- colnames(NRC_Emotion_Lexicon)
names <-gsub(".* ","",names)
names <-gsub("[()]","",name)
names
colnames(NRC_Emotion_Lexicon) <- names
NRC_Emotion_Lexicon %>% select(-"en...1")
NRC_Emotion_Lexicon %>% select(es, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
NRC_Emotion_Lexicon %>% select(es, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
NRC_Emotion_Lexicon
NRC_Emotion_Lexicon <- read_excel("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
names <- colnames(NRC_Emotion_Lexicon)
names <-gsub(".* ","",names)
names <-gsub("[()]","",name)
colnames(NRC_Emotion_Lexicon) <- names
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(-"en...1")
NRC_Emotion_Lexicon %>% select(es, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
custom_lexicon <- NRC_Emotion_Lexicon %>% select(es, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
custom_lexicon
twitter_sentiment <- function(df, language, undesirable_words){
stopwords <- data.frame(word = stopwords::stopwords(language = language, source = "stopwords-iso"))
stopwords$lexicon <- language
stopwords <- as_tibble(stopwords)
NRC_Emotion_Lexicon <- read_excel("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
names <- colnames(NRC_Emotion_Lexicon)
names <-gsub(".* ","",names)
names <-gsub("[()]","",name)
colnames(NRC_Emotion_Lexicon) <- names
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(-"en...1")
custom_lexicon <- NRC_Emotion_Lexicon %>% select(language, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3) %>% #Words like "ah" or "oo" used in music
anti_join(stopwords)
youtube_nrc <- nrc_tidy %>%
inner_join(custom_lexicon)
}
d <- twitter_sentiment(basic_tweets, "es", c("shit", "fuck"))
View(d)
twitter_sentiment <- function(df, language, undesirable_words){
stopwords <- data.frame(word = stopwords::stopwords(language, source = "stopwords-iso"))
stopwords$lexicon <- language
stopwords <- as_tibble(stopwords)
NRC_Emotion_Lexicon <- read_excel("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
names <- colnames(NRC_Emotion_Lexicon)
names <-gsub(".* ","",names)
names <-gsub("[()]","",name)
colnames(NRC_Emotion_Lexicon) <- names
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(-"en...1")
custom_lexicon <- NRC_Emotion_Lexicon %>% select(language, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3) %>% #Words like "ah" or "oo" used in music
anti_join(stopwords)
youtube_nrc <- nrc_tidy %>%
inner_join(custom_lexicon)
}
d <- twitter_sentiment(basic_tweets, "es", c("shit", "fuck"))
View(d)
View(custom_lexicon)
View(get_sentiments("nrc"))
View(custom_lexicon)
custom_lexicon$sent <- rowsum(custom_lexicon)
View(custom_lexicon)
custom_lexicon$sent <- rowSums(custom_lexicon[,2:9])
custom_lexocim[custom_lexicon$sent >0,]
custom_lexicon[custom_lexicon$sent >0,]
custom_lexicon[rowSums(custom_lexicon[,2:9]) >0,]
twitter_sentiment <- function(df, language, undesirable_words){
stopwords <- data.frame(word = stopwords::stopwords(language, source = "stopwords-iso"))
stopwords$lexicon <- language
stopwords <- as_tibble(stopwords)
NRC_Emotion_Lexicon <- read_excel("source/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx")
names <- colnames(NRC_Emotion_Lexicon)
names <-gsub(".* ","",names)
names <-gsub("[()]","",name)
colnames(NRC_Emotion_Lexicon) <- names
NRC_Emotion_Lexicon <- NRC_Emotion_Lexicon %>% select(-"en...1")
custom_lexicon <- NRC_Emotion_Lexicon %>% select(language, Anger, Anticipation, Disgust,Fear, Joy, Sadness, Surprise, Trust)
colnames(custom_lexicon) <- c("word", "Anger", "Anticipation", "Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust" )
custom_lexicon <- custom_lexicon[rowSums(custom_lexicon[,2:9]) >0,]
#Create tidy text format: Unnested, Unsummarized, -Undesirables, Stop and Short words
nrc_tidy <- df %>%
unnest_tokens(word, text) %>% #Break the lyrics into individual words
filter(!word %in% undesirable_words) %>% #Remove undesirables
filter(!nchar(word) < 3) %>% #Words like "ah" or "oo" used in music
anti_join(stopwords)
youtube_nrc <- nrc_tidy %>%
inner_join(custom_lexicon)
}
d <- twitter_sentiment(basic_tweets, "es", c("shit", "fuck"))
View(d)
library(ComTxt)
tweets_df_clean <- readRDS("C:/Users/LocalAdmin/OneDrive - Universitat Autònoma de Barcelona/ComTxt/source/tweets_df_clean.Rds")
library(udpipe)
basic_tweets <- readRDS("C:/Users/LocalAdmin/OneDrive - Universitat Autònoma de Barcelona/ComTxt/source/basic_tweets.Rds")
df <- basic_tweets
df <- data.frame(doc_id = df$status_id, text = df$text, stringsAsFactors = FALSE)
udpipe_df <-  udpipe(df, object = "es", trace = 10)
?udpipe
udpipe_df <-  udpipe(df, object = "spanish", trace = 10)
View(udpipe_df)
View(df)
udpipe_df <-  udpipe(df, object = "spanish", trace = 10)
udpipe_df <-  udpipe(df, object = "spanish", trace = 10)
library(udpipe)
udpipe_df <-  udpipe(df, object = "spanish", trace = 10)
