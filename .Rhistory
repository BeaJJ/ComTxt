stringi::stri_trans_char(df$hashtags[[1]], "#")
paste0(df$hashtags[[1]], "#")
paste0("#", df$hashtags)
paste0("#", df$hashtags[[1]])
df$hashtags <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
View(df)
i
i = 1
paste0("#", df$hashtags[[i]])
df <- pre_df
paste0("#", df$hashtags[[i]])
paste(paste0("#", df$hashtags[[i]]), collapse = " "))
paste(paste0("#", df$hashtags[[i]]), collapse = " ")
tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
for(i in 1:nrow(df)){
df$hashtags <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
View(df)
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df <- pre_df
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
View(df)
df$hashtags <- gsub("#na", NA, df$hashtags)
View(df)
class(df$hashtags)
library(quanteda.textplots)## Semantic network
library(quanteda)
library(RColorBrewer)
library(dplyr)
hash_dfm <- dfm(df$hashtags) # Document-feature matrix
toptag <- names(topfeatures(hash_dfm, 30)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
textplot_network(topgat_fcm, min_freq = 0.1, edge_alpha = 0.5, edge_size = 5)
##draw semantic network plot
quanteda::textplot_network(fcm_1, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
##draw semantic network plot
quanteda::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags) # Document-feature matrix
View(df)
hashtags_network <- function(df, n_top = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags) # Document-feature matrix
toptag <- names(topfeatures(hash_dfm, n_top)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
##draw semantic network plot
quanteda::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
hashtags_network(pre_df)
all_hashtags <- tolower(unlist(df$hashtags))
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
kable(head(hashtag_frequencies, 10), "simple", caption = "Top 10 locations")
tmp <- sort(table(df$city), decreasing = TRUE)
kable(head(hashtag_frequencies, 10), "simple", caption = "Top 10 locations")
kable(head(tmp, 10), "simple", caption = "Top 10 locations")
colnames(tmp) <- c("city", "freq")
top_n = 10
print(kable(head(hashtag_frequencies, top_n), "simple", caption = "Top 10 locations"))
df <- pre_df
all_hashtags <- tolower(unlist(df$hashtags))
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
print(kable(head(hashtag_frequencies, top_n), "simple", caption = "Top 10 locations"))
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
hashtags_df <- function(df, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
hashtags_df(pre_df, top_n = 15)
hash_dfm <- dfm(df$text) # Document-feature matrix
toptag <- names(topfeatures(hash_dfm, top_n))
topfeatures(hash_dfm, top_n)
df <- pre_df
hash_dfm <- dfm(df$text) # Document-feature matrix
topfeatures(hash_dfm, top_n)
class(topfeatures(hash_dfm, top_n))
toptag <- sort(topfeatures(hash_dfm, top_n), decreasing = TRUE)
print(kable(toptag, "simple", caption = paste("Top", top_n, "keyword")))
toptag <- as.data.frame(toptag)
colnames(toptag) <- c("keyword", "freq")
toptag <- as.data.frame(toptag)
colnames(toptag) <- c("keyword", "freq")
toptag <- data.frame(toptag)
colnames(toptag) <- c("keyword", "freq")
class(toptag)
toptag
library(dplyr)
df <- tibble::rownames_to_column(toptag, "freq")
df
df <- tibble::rownames_to_column(toptag, "keyword")
df
toptag <- tibble::rownames_to_column(toptag, "keyword")
colnames(toptag) <- c("keyword", "freq")
print(kable(toptag, "simple", caption = paste("Top", top_n, "keyword")))
df <- pre_df
keyword_df <- function(df, top_n = 10){
hash_dfm <- dfm(df$text) # Document-feature matrix
toptag <- sort(topfeatures(hash_dfm, top_n), decreasing = TRUE)
toptag <- data.frame(toptag)
library(dplyr)
toptag <- tibble::rownames_to_column(toptag, "keyword")
colnames(toptag) <- c("keyword", "freq")
print(kable(toptag, "simple", caption = paste("Top", top_n, "keyword")))
}
keyword_df(df, top_n = 10)
df <- pre_df
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
hash_dfm <- dfm(toks) # Document-feature matrix
toptag <- sort(topfeatures(hash_dfm, top_n), decreasing = TRUE)
toptag <- tibble::rownames_to_column(toptag, "keyword")
toptag <- data.frame(toptag)
toptag <- tibble::rownames_to_column(toptag, "keyword")
colnames(toptag) <- c("keyword", "freq")
print(kable(toptag, "simple", caption = paste("Top", top_n, "keyword")))
keyword_df <- function(df, top_n = 10){
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
hash_dfm <- dfm(toks) # Document-feature matrix
toptag <- sort(topfeatures(hash_dfm, top_n), decreasing = TRUE)
toptag <- data.frame(toptag)
toptag <- tibble::rownames_to_column(toptag, "keyword")
colnames(toptag) <- c("keyword", "freq")
print(kable(toptag, "simple", caption = paste("Top", top_n, "keyword")))
}
keyword_df(df, top_n = 10)
df <- pre_df
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags) # Document-feature matrix
fcmat <- fcm(hash_dfm , context = "window", tri = FALSE) # Feature-occurance matrix which shows the network structure
hash_dfm <- tokens(df$hashtags) # Document-feature matrix
fcmat <- fcm(hash_dfm , context = "window", tri = FALSE) # Feature-occurance matrix which shows the network structure
hub = "art"
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
hub = "#art"
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
hashtag_remove = "cultura"
hashtags_network <- function(df, hashtag_remove = "cultura", n_top = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags)
hash_dfm <- dfm_remove(dfmat, hashtag_remove)# Document-feature matrix
toptag <- names(topfeatures(hash_dfm, n_top)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
topgat_fcm <- tokens_remove(toks, pattern = stopwords("en"))
##draw semantic network plot
quanteda::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
hashtags_network(pre_df)
hashtags_network <- function(df, hashtag_remove = "cultura", n_top = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags)
hash_dfm <- dfm_remove(hash_dfm, hashtag_remove)# Document-feature matrix
toptag <- names(topfeatures(hash_dfm, n_top)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
topgat_fcm <- tokens_remove(toks, pattern = stopwords("en"))
##draw semantic network plot
quanteda::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
hashtags_network(pre_df)
df <- pre_df
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags)
hash_dfm <- dfm_remove(hash_dfm, hashtag_remove)# Document-feature matrix
toptag <- names(topfeatures(hash_dfm, n_top)) # Most important hashtags; we dont want to plot every hashtag
n_top = 40
toptag <- names(topfeatures(hash_dfm, n_top)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
##draw semantic network plot
quanteda::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
hashtags_network <- function(df, hashtag_remove = "#cultura", n_top = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- dfm(df$hashtags)
hash_dfm <- dfm_remove(hash_dfm, hashtag_remove)# Document-feature matrix
toptag <- names(topfeatures(hash_dfm, n_top)) # Most important hashtags; we dont want to plot every hashtag
tag_fcm <- fcm(hash_dfm) # Feature-occurance matrix which shows the network structure
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) # Filter results so that we plot only 50 top hashtags
##draw semantic network plot
quanteda::textplot_network(topgat_fcm, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
hashtags_network(pre_df)
hashtags_network_hub <- function(df, n_top = 40){
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- tokens(df$hashtags) # Document-feature matrix
fcmat <- fcm(hash_dfm , context = "window", tri = FALSE) # Feature-occurance matrix which shows the network structure
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
}
hashtags_network_hub(pre_df)
keyword_network <- function(df, n_top = 40){
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
#toks <- tokens_wordstem(toks, language = language)
## A feature co-occurrence matrix
fcmat <-quanteda::fcm(toks, context = "window", tri = FALSE)
##reduce only top words
feat <-  names(topfeatures(fcmat, n_top))
##subset top 40 words
fcm_1 <- fcm_select(fcmat, pattern = feat)
##draw semantic network plot
quanteda::textplot_network(fcm_1, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
keyword_network(pre_df)
hashtags_df(pre_df)
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies[1]
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
df <- pre_df
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies[1]
hashtag_frequencies[!row.names(hashtag_frequencies)%in% "#cultura",]
hashtag_frequencies[!row.names(hashtag_frequencies)%in% "#cultura"]
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies[!row.names(hashtag_frequencies)%in% hashtag_remove]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
hashtags_df(pre_df)
hashtags_df(pre_df, "#cultura")
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies <- hashtag_frequencies[!row.names(hashtag_frequencies)%in% hashtag_remove]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
hashtags_df(pre_df, "#cultura")
df <- pre_df
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
View(hashtag_frequencies)
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
#hashtag_frequencies <- hashtag_frequencies[!row.names(hashtag_frequencies)%in% hashtag_remove]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
View(hashtag_frequencies)
hashtags_df(pre_df)
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
all_hashtags <- gsub("#Na", "", all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies <- hashtag_frequencies[!row.names(hashtag_frequencies)%in% hashtag_remove]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
hashtags_df(pre_df, "#cultura")
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
all_hashtags <- gsub("#NA", "", all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies <- hashtag_frequencies[!row.names(hashtag_frequencies)%in% hashtag_remove]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
hashtags_df(pre_df, "#cultura")
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
#all_hashtags <- gsub("#NA", "", all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies <- hashtag_frequencies[!row.names(hashtag_frequencies)%in% c(hashtag_remove, "#NA")]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
hashtags_df(pre_df, "#cultura")
keyword_df <- function(df, keyword_remove, top_n = 10){
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
hash_dfm <- dfm(toks) # Document-feature matrix
hash_dfm <- dfm_remove(hash_dfm, keyword_remove)
toptag <- sort(topfeatures(hash_dfm, top_n), decreasing = TRUE)
toptag <- data.frame(toptag)
toptag <- tibble::rownames_to_column(toptag, "keyword")
colnames(toptag) <- c("keyword", "freq")
print(kable(toptag, "simple", caption = paste("Top", top_n, "keyword")))
}
keyword_df(pre_df, "cultura")
df <- readRDS("F:/Bea/PostDoc/RESEARCH/ComTxt/data-raw/basic_tweets.Rds")
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
## A feature co-occurrence matrix
fcmat <-quanteda::fcm(toks, context = "window", tri = FALSE)
hub = "arte"
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
tmp
sort(tmp, decreasing = T)
sort(tmp, decreasing = TRUE)
sort(tmp[tmp$degree,], decreasing = TRUE)
sort(tmp$degree, decreasing = TRUE)
sort(tmp$degree, decreasing = FALSE)
View(tmp)
hub = "literatura"
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
View(tmp)
keyword_network <- function(df, keyword_remove, n_top = 40){
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
#toks <- tokens_wordstem(toks, language = language)
## A feature co-occurrence matrix
fcmat <-quanteda::fcm(toks, context = "window", tri = FALSE)
##reduce only top words
feat <-  names(topfeatures(fcmat, n_top))
##subset top 40 words
fcm_1 <- fcm_select(fcmat, pattern = feat)
fcm_1 <- fcm_remove(fcm_1, keyword_remove)
##draw semantic network plot
quanteda::textplot_network(fcm_1, min_freq = 1, edge_color = "grey",vertex_color ="#538797")
}
keyword_network(df)
keyword_network(df, keyword_remove = "cultura")
hub = "que"
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
View(tmp)
nodes_degree <- tmp[tmp$nodes == hub, ]$degree
high_nodes <- tmp[tmp$degree > nodes_degree, ]$nodes
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
fcm_local <- fcm_remove(fcm_local, c(keyword_remove, high_nodes))
keyword_remove = "cultura"
fcm_local <- fcm_remove(fcm_local, c(keyword_remove, high_nodes))
##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
##reduce only top words
feat <-  names(topfeatures(fcm_local, n_top))
n_top <- 40
##reduce only top words
feat <-  names(topfeatures(fcm_local, n_top))
##subset top 40 words
fcm_local <- fcm_select(fcm_local, pattern = feat)
##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
keyword_network_hub <- function(df, hub, n_top){
toks <- tokens(df$text, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, padding = FALSE, min_nchar =3)
## A feature co-occurrence matrix
fcmat <-quanteda::fcm(toks, context = "window", tri = FALSE)
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
nodes_degree <- tmp[tmp$nodes == hub, ]$degree
high_nodes <- tmp[tmp$degree > nodes_degree, ]$nodes
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
fcm_local <- fcm_remove(fcm_local, high_nodes)
##reduce only top words
feat <-  names(topfeatures(fcm_local, n_top))
##subset top 40 words
fcm_local <- fcm_select(fcm_local, pattern = feat)
##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
}
keyword_network_hub(df, hub = "que")
keyword_network_hub(df, hub = "que", n_top = 40)
for(i in 1:nrow(df)){
df$hashtags[[i]] <- tolower(paste(paste0("#", df$hashtags[[i]]), collapse = " "))
}
df$hashtags <- gsub("#na", NA, df$hashtags)
df <- df[!is.na(df$hashtags),]
hash_dfm <- tokens(df$hashtags) # Document-feature matrix
fcmat <- fcm(hash_dfm , context = "window", tri = FALSE) # Feature-occurance matrix which shows the network structure
hub = "#literatura"
##selecting keywords inside fcmat
tmp <- fcmat[, hub]
tmp <- convert(tmp, to = "data.frame")
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]
nodes_degree <- tmp[tmp$nodes == hub, ]$degree
high_nodes <- tmp[tmp$degree > nodes_degree, ]$nodes
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
fcm_local <- fcm_remove(fcm_local, high_nodes)
##reduce only top words
feat <-  names(topfeatures(fcm_local, n_top))
fcm_local <- fcm_select(fcmat, pattern = c(tmp, hub))
##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797")
hashtags_df <- function(df, hashtag_remove, top_n = 10){
all_hashtags <- tolower(unlist(df$hashtags))
all_hashtags <- paste0("#",all_hashtags)
#all_hashtags <- gsub("#NA", "", all_hashtags)
hashtag_frequencies <- sort(table(all_hashtags), decreasing = TRUE)
hashtag_frequencies <- hashtag_frequencies[!row.names(hashtag_frequencies)%in% c(hashtag_remove, "#NA")]
print(kable(head(hashtag_frequencies, top_n), "simple", caption = paste("Top", top_n, "hashtags")))
}
library(ComTxt)
tweets_df_clean <- readRDS("F:/Bea/PostDoc/RESEARCH/ComTxt/data-raw/tweets_df_clean.Rds")
View(tweets_df_clean)
tweets_df_clean$geo_coord[[1]]
tweets_df_clean$geo_coord[[1]][1]
df <- tweets_df_clean
for(i in 1:nrow(df)){
dd <- df$geo_coord[[i]][2]
df$geo_coord[[i]][[2]] <- df$geo_coord[[i]][[1]]
df$geo_coord[[i]][[1]] <- dd
}
i =
i
i
dd <- df$geo_coord[[i]][2]
dd
df$geo_coord[[i]][[1]]
df$geo_coord[[1]][[1]]
df$geo_coord[[2]]
df$geo_coord[[1]]
is.na(df$geo_coord[[2]])
length(df$geo_coord[[2]])
length(df$geo_coord[[2]]) == 0
length(df$geo_coord[[1]]) == 0
i = 1
i = 2
if(length(df$geo_coord[[i]]) = 0){
df$geo_coord[[i]][[1]] <- NA
df$geo_coord[[i]][[2]] <- NA
}
if(length(df$geo_coord[[i]]) == 0){
df$geo_coord[[i]][[1]] <- NA
df$geo_coord[[i]][[2]] <- NA
}
df$geo_coord[[2]]
i
dd[[i]] <- df$geo_coord[[i]][2]
dd
dd <- df$geo_coord[[i]][2]
dd
for(i in 1:nrow(df)){
if(length(df$geo_coord[[i]]) == 0){
df$geo_coord[[i]][[1]] <- NA
df$geo_coord[[i]][[2]] <- NA
}
dd <- df$geo_coord[[i]][1]
df$geo_coord[[i]][[1]] <- df$geo_coord[[i]][[2]]
df$geo_coord[[i]][[2]] <- dd
}
