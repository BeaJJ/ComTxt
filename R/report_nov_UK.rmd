---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

## 1. Load the excel file 

The preprocess includes reducing categories such as the birth year into age such as the 20s to 70s. Another column is added for languages and User ID to increase convenience for analysis. After the data is reformed by responds as the first column and variables as followed columns. We also add the column of question number such as 'Q.3' for 'what is culture', 'Q.4' for 'importance of culture', 'Q.5' for 'different culture' , and 'Q.6' for 'COVID'. This arrangement is matched with Spanish pilot survey data. 


```{r message=FALSE, warning=FALSE}
##ADD YOUR PREPORCESS R SCRIPT ADDRESS
##source("preprocess_original.R")
```


## 2. Subset the data frame according to language and question number

First let's upload the R packages for text analysis. 'Quanteda' packages is for semantic analysis, 'wordcloud' packages is for words cloud, and all the others are for arranging data frame. Before conducting analysis select language for text analysis (language_set) and subset the data by question number (question_id). In the shiny application, you can choose from the selection bar. Here in this report, I select the langauge to 'English (en)' and question about 'what is culture (Q.3)'. Plus the emoticons are deleted. Now we are ready to counduct the analysis. 

```{r message=FALSE}
library(quanteda)
library(RColorBrewer)
library(wordcloud)
library(dplyr)
library(data.table)

##Subset the data language 
language_set = "en"
question_id = "Q.3" ##"Q.4" "Q.5", "Q.6"

df <- raw_uk_df[raw_uk_df$Language == language_set & raw_uk_df$Question_Number == question_id, ]

##delete the emoticon and word "etc" 
df$Answer <- gsub("[<]+.*[>]", "", df$Answer)
```

## 3. Words frequency cloud 

Using quanteda package in R the response of informants is defined into the corpus and the respondent's information is set as variables including age and gender. 

```{r}
##Make the corpus
corp <-  corpus(df$Answer, docvars = data.frame(age = df$Age, gender = df$Gender, education = df$Education, occupation = df$Occupation, nationality = df$Nationality, residence = df$Residence, citizenship = df$Citizenship, religion = df$Religion))
```

To analysis, the word frequencies and draw words cloud the corpus format data is turned into a 'data frame matrix' by using quanteda packages. Here we process the answers by natural language processing to clean the text before analysis. We filter out the list of common words named 'stopwords'. The English stopwords list is developed from the stopwords package which contains 1298 words such as 'about, actually' and so on. Also, we remove the punctuations and return the text to lower case. Stemming is the process that returns the word to its base form such as "is" to "be". We select the words which are composed of at least three characters to delete the noise in the analysis. After all this natural language processing we draw the word cloud with the top 300 words.

```{r warning=FALSE, message=FALSE}
##Make Data frame Matrix for general (without categories)
dfmat <- dfm(corp, remove = stopwords::stopwords(language_set, source = "stopwords-iso"), remove_punct = TRUE, tolower = TRUE, verbose = TRUE) %>%
  dfm_wordstem(language = language_set) %>%
  dfm_select(min_nchar = 3) 

##Text plot for General word cloud
textplot_wordcloud(dfmat, rotation = 0.25,
  color = RColorBrewer::brewer.pal(8, "Dark2"), max_words = 300, min_count = 1)
```

For the comparison words clouds, we conduct the same process but we add groups such according to gender or age.  

```{r warning=FALSE, message=FALSE}
##Data Frame Matrix with Categories (Gender / Age)
dfmat2 <-dfm(corpus_subset(corp, gender != "Unknown"), remove = stopwords::stopwords(language_set, source = "stopwords-iso"), remove_punct = TRUE,  groups = "gender", tolower = TRUE, verbose = TRUE) %>%
  dfm_wordstem(language = language_set) %>%
  dfm_select(min_nchar = 3)

textplot_wordcloud(dfmat2, rotation = 0.25,comparison = TRUE, max_words = 300, min_count = 1, color = brewer.pal(8,"Dark2"))

dfmat2 <- dfm(corpus_subset(corp, age != "Unknown"), remove = stopwords::stopwords(language_set, source = "stopwords-iso"), remove_punct = TRUE, groups = "age", tolower = TRUE, verbose = TRUE) %>%
  dfm_wordstem(language = language_set) %>%
  dfm_select(min_nchar = 3)

textplot_wordcloud(dfmat2, rotation = 0.25,comparison = TRUE, max_words = 300, min_count = 1, color = brewer.pal(8,"Dark2"))

```

## 4. Semantic Network 

The first step for semantic network analysis is tokenizing the text and build the feature co-occurrence matrix by using quanteda package. Here also we process natural language processing including removing punctuation, turning into lower case, filtering out the stopwords, selecting words at least with three characters, and finally stemming. After this preprocess we build the feature co-occurrence matrix. 

```{r message=FALSE}
toks <- tokens(df$Answer, remove_punct = TRUE, remove_symbols = TRUE, verbose = TRUE)
toks <- tokens_tolower(toks)
toks <- tokens_remove(toks, pattern = stopwords::stopwords(language_set, source = "stopwords-iso"), padding = FALSE, min_nchar =3)
toks <- tokens_wordstem(toks, language = language_set)

## A feature co-occurrence matrix
fcmat <-quanteda::fcm(toks, context = "window", tri = FALSE)
```

### 4.1 General Semantic Network analysis 

For a clear picture of the analysis, we select the top 40 words which co-occur the most with other words using the function topfeatures in quanteda. . 

```{r}
##reduce only top words
feat <-  names(topfeatures(fcmat, 40))
```

We select the top 40 words and make a table output with words and their degree. In the network analysis 'degree' means how many times this word links to other words.

```{r}
## draw the table
data_fcmat <- data.frame(name = rownames(fcmat))
rownames(data_fcmat) <- NULL
data_fcmat$degree <- rowSums(fcmat)
table_fcmat <- data_fcmat
##see the tale
fcm_tb <- table_fcmat[table_fcmat$name %in% feat, ]

fcm_tb <- fcm_tb %>% arrange(desc(degree))

reactable::reactable(fcm_tb,
            filterable = TRUE,
            searchable = TRUE,
            bordered = TRUE,
            striped = TRUE,
            highlight = TRUE,
            showSortable = TRUE,
            defaultSortOrder = "desc",
            defaultPageSize = 25,
            showPageSizeOptions = TRUE,
            pageSizeOptions = c(25, 50, 75, 100, 200)
        )
```


```{r}
##subset top 40 words
fcm_1 <- fcm_select(fcmat, pattern = feat)
##draw semantic network plot
quanteda::textplot_network(fcm_1, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797",  vertex_labelsize = (rowSums(fcm_1)/min(rowSums(fcm_1)))*0.5, edge_size = 3)
```

### 4.2 Local Hub Semantic analysis 
 
Next, we analyze the local hub for semantic network analysis meaning that we subset the network analysis around the keywords. As an example, we choose the keywords "art" which appears as one of the most important keywords in general semantic network analysis above. After that, we also delete the nodes which have a higher degree than the keyword "art" to make the keywords the central words inside the network.  

```{r}
##selecting keywords inside fcmat
tmp <- fcmat[, "art"]
tmp <- as.data.table(tmp)
colnames(tmp) <- c("nodes", "degree")
tmp <- tmp[tmp$degree > 0, ]

## deleteing the higher degree nodes then key word degree
nodes_degree <- table_fcmat[table_fcmat$name == "art", ]$degree
high_nodes <- table_fcmat[table_fcmat$degree > nodes_degree, ]$name

if(high_nodes %in% tmp$nodes  == TRUE){
  tmp <- tmp[ !tmp$nodes %in% high_nodes, ]$nodes
} else {
  tmp <- tmp$nodes
}
fcm_local <- fcm_select(fcmat, pattern = c(tmp, "art"))
```

Here is the table output and the semantic network plot for the key word "art". 

```{r}
##table data
nodes_lh <- rownames(fcm_local)
fcm_local_tb <- table_fcmat[table_fcmat$name %in% nodes_lh, ]
fcm_local_tb <- fcm_local_tb %>% arrange(desc(degree))
reactable::reactable(fcm_local_tb,
            filterable = TRUE,
            searchable = TRUE,
            bordered = TRUE,
            striped = TRUE,
            highlight = TRUE,
            showSortable = TRUE,
            defaultSortOrder = "desc",
            defaultPageSize = 25,
            showPageSizeOptions = TRUE,
            pageSizeOptions = c(25, 50, 75, 100, 200)
        )

##text plot
quanteda::textplot_network(fcm_local, min_freq = 0.1, edge_color = "grey",vertex_color ="#538797",  vertex_labelsize = (rowSums(fcm_local)/min(rowSums(fcm_local)))*0.3, edge_size = 3)
```

### 4.3 Key Word In Context 

Inside the local hub of art, we are interested in some few words and how they appear in the response actually. KWIC analysis shows five words in front and after the keyword in the sentence. For example, we set the keyword 'languag'. The result below is the sentences that use that keyword 'languag' and which five words are used together in front and after.

```{r}
kwic(corp, pattern = "languag", window = 5, valuetype = "regex")
```

